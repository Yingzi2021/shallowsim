# My Plan 

(1) background information

> - LLM inference process in general (ðŸ‘Œ)
>   - prefill (Prompt) & decode (Generation) 
>   - **memory bound(KV cache)**
>
> - Different parallelism strategies (ðŸ‘Œ)
>   - Data parallelism
>   - Tensor parallelism
>   - sequence parallelism
>   - expert parallelism
>
> - Different attention mechanism (ðŸ‘Œ)
>
>   - Multi-head attention (MHA)
>
>   - Multi-query attention(MQA)
>
>   - Group query attention(GQA)
>
>   - Multi-head latent attention(MLA)
>
> - Architecture for specific LLMs
>
>   - DeepSeek: MoE & MLA (ðŸ‘Œ)
>
>   - Llama-3(ðŸ‘Œ)
>
>   - Qwen series(ðŸ‘Œ) 
>
> 

(2) Run & analyze the code (ðŸ‘Œ)



(3) Add the implementation for GPT / Llama-like LLM
