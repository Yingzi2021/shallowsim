# My Plan 

(1) Learn something about the process of LLM inference

> Core: prefill (Prompt) & decode (Generation) --> memory bound
>
> - LLM inference in general
>
>   https://github.com/xlite-dev/Awesome-LLM-Inference
>
>   https://huggingface.co/docs/transformers/v4.44.1/llm_optims#llm-inference-optimization
>
>   https://www.snowflake.com/guides/llm-inference/
>   https://arxiv.org/pdf/2404.14294
>
> - Deepseek (MLA ?)
>
> - GPT-like 
>
> - Llama-like

(2) Pipeline parallelism

https://zhuanlan.zhihu.com/p/613196255

(3) Run & analyze the code



(4) Add the implementation for GPT / Llama-like LLM

