# Useful resources and links

[A Brief History of LLMs --- From Transformers (2017) to DeepSeek-R1 (2025)](https://medium.com/@lmpo/a-brief-history-of-lmms-from-transformers-2017-to-deepseek-r1-2025-dae75dd3f59a)

[Deep Generative Models](https://deepgenerativemodels.github.io/)

> Stanford CS236 - Fall 2023
>
> https://deepgenerativemodels.github.io/notes/autoregressive/

[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#

[⭐Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)

LLM inference 

[⭐Decoding Strategies in Large Language Models -- A Guide to Text Generation From Beam Search to Nucleus Sampling](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)

[How GPT3 Works - Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)

[⭐The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)

[Understanding the LLM Inference Workload - Mark Moyou, NVIDIA](https://www.youtube.com/watch?v=z2M8gKGYws4)

[⭐Mastering LLM Techniques: Inference Optimization](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)

https://github.com/wdndev/llm_interview_note/tree/main

> Chinese 

KV-Cache

[The KV Cache: Memory Usage in Transformers](https://www.youtube.com/watch?v=80bIUggRJf4)

[Transformers KV Caching Explained](https://medium.com/%40joaolages/kv-caching-explained-276520203249)

Attention

[DeepSeek/MLA Topics](https://github.com/xlite-dev/Awesome-LLM-Inference?tab=readme-ov-file#mla)

https://github.com/wdndev/llm_interview_note/tree/main

DeepSeek MoE

[⭐Transformer vs. Mixture of Experts in LLMs](https://www.dailydoseofds.com/p/transformer-vs-mixture-of-experts-in-llms/)

https://zhuanlan.zhihu.com/p/18565423596